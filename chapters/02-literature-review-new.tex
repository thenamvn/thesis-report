\chapter{LITERATURE REVIEW}
\label{chap:lit-review}

This chapter provides a comprehensive review of the scholarly and technical literature pertinent to the core components of the TEKUTOKO platform. It examines four key domains: gamification in education, AI-driven content generation, anti-cheating mechanisms in online assessments, and modern architectural paradigms for web applications. The chapter culminates in a comparative analysis of existing platforms and a synthesis of identified research gaps that this thesis aims to address.

\section{Overview of the Topics}
\label{sec:lit-overview}

The convergence of digital technologies and educational practices has fundamentally transformed how knowledge is created, shared, and assessed in the 21st century. This literature review explores the theoretical foundations and empirical evidence supporting the integration of four interconnected technological domains that form the basis of modern e-learning platforms: gamification, artificial intelligence, online proctoring, and microservice architectures.

\subsection{Gamification in Educational Technology}

Gamification is defined as the application of game-design elements and principles in non-game contexts to engage users and solve problems. In education, this approach is grounded in motivational psychology, particularly Self-Determination Theory (SDT), which posits that intrinsic motivation is fostered by satisfying three innate psychological needs: \textbf{autonomy} (the desire to control one's own actions), \textbf{competence} (the need to feel effective and master challenges), and \textbf{relatedness} (the urge to connect with others) \citep{deci2020}.

A well-designed gamified system provides learners with choices (autonomy), offers achievable challenges with clear feedback and progress indicators (competence), and facilitates social interaction through leaderboards and team activities (relatedness). The theoretical foundation of gamification in education extends beyond SDT to encompass flow theory, which explains how optimal challenge levels can create immersive, engaging experiences \citep{sailer2020}.

A growing body of empirical research supports the efficacy of gamification in educational contexts. Recent meta-analyses have examined numerous studies and found that gamification generally yields positive effects on user engagement, motivation, and learning outcomes \citep{sailer2020, krath2021}. However, these reviews also caution that the context and implementation are crucial; poorly designed systems can be perceived as manipulative and may even decrease intrinsic motivation.

Prominent platforms like \textbf{Duolingo} have successfully demonstrated the power of gamification by using streaks, experience points (XP), and competitive leagues to create a habit-forming language-learning experience. The platform's success demonstrates how gamification elements, when thoughtfully integrated, can sustain long-term user engagement \citep{hamari2022}. Similarly, \textbf{Kahoot!} has transformed classroom quizzes into fast-paced, competitive games, generating high levels of excitement and participation among students.

\subsection{AI and Large Language Models in Education}

The automatic generation of educational content has evolved significantly over the past decade. Early systems relied on static templates and rule-based algorithms, which were limited in their flexibility and linguistic sophistication. The advent of generative AI, powered by Large Language Models (LLMs) built on the Transformer architecture (e.g., Google's Gemini, OpenAI's GPT series), has revolutionized this field \citep{brown2020}.

These models can generate fluent, contextually appropriate text for a wide range of applications, including Automatic Question Generation (AQG). LLMs can be prompted to create diverse assessment items, including multiple-choice questions, short-answer prompts, and even complex scenarios. A particularly valuable capability is their ability to generate plausible "distractors" (incorrect options) for multiple-choice questions, a task that is often challenging and time-consuming for human educators \citep{kurni2023}.

Recent research has explored the application of ChatGPT and similar models in educational contexts, identifying both opportunities and challenges. These systems can assist with personalized learning, automated grading, intelligent tutoring, and content generation \citep{kasneci2023}. However, a key challenge in using AI for content generation is ensuring the quality, accuracy, and pedagogical soundness of the output. LLMs can sometimes "hallucinate" incorrect information or generate questions that are ambiguous or biased \citep{ji2023}.

Therefore, a \textbf{"human-in-the-loop"} approach is widely considered essential. This model, which should be implemented in modern AI-enhanced educational systems, allows educators to review, edit, and approve AI-generated content before it is presented to learners, balancing the efficiency of automation with the need for academic rigor and pedagogical expertise \citep{baidoo2023}. The sustainable adoption of generative AI in higher education requires comprehensive frameworks that address technical, pedagogical, ethical, and institutional dimensions \citep{chiu2024}.

\subsection{Online Proctoring and Academic Integrity}

The credibility of online education hinges on the integrity of its assessment methods. The COVID-19 pandemic dramatically accelerated the adoption of online learning and remote assessments, exposing significant challenges in maintaining academic integrity in digital environments \citep{hodges2020}. The spectrum of anti-cheating solutions ranges from simple browser-based measures to sophisticated AI-powered proctoring systems.

\textbf{Browser-based security measures} represent the first line of defense. These include disabling copy-paste functionality, locking the test to a full-screen window, and monitoring for tab or window focus changes using browser APIs. Research has shown that even these basic measures can significantly deter casual or opportunistic cheating while maintaining relatively low technical overhead and respecting user privacy \citep{dawson2020}.

More advanced systems involve \textbf{AI-powered online proctoring}, which typically uses webcam and microphone feeds to monitor students during an exam. These systems can detect suspicious behaviors such as looking away from the screen, the presence of another person, or the use of unauthorized materials. Comprehensive reviews of AI-based proctoring systems have documented their evolution from simple rule-based systems to sophisticated machine learning approaches \citep{nigam2021}.

However, these solutions are often expensive and raise significant privacy and ethical concerns among students, which can lead to increased test anxiety and may disadvantage students with limited resources or privacy \citep{ullah2021}. Research on implementation motivational factors has revealed that while institutions recognize the need for proctoring, student acceptance varies significantly based on privacy concerns and perceived fairness \citep{gonzalez2023}.

The challenge for modern e-learning platforms is to develop proctoring approaches that balance the need for academic integrity with concerns about privacy, accessibility, cost, and user experience. Lightweight, browser-based approaches that focus on behavior monitoring rather than invasive surveillance represent a promising middle ground for many educational contexts, particularly for low- to medium-stakes assessments \citep{reisenwitz2020}.

\subsection{Microservice Architectures for Scalable Systems}

Software architecture is a critical determinant of a web application's ability to scale, evolve, and remain resilient under varying loads and changing requirements. A \textbf{monolithic architecture} bundles all application components into a single, tightly coupled unit. While this approach can be simpler for small projects, it becomes unwieldy as the application grows, leading to development bottlenecks, complex deployments, and a single point of failure \citep{newman2021}.

In contrast, a \textbf{microservice architecture} structures an application as a collection of small, independent services, each responsible for a specific business capability. These services communicate with each other over a network, typically using lightweight protocols like REST APIs or message queues. This approach offers several key benefits relevant to the EdTech domain:

\begin{itemize}
    \item \textbf{Independent Scalability:} Services can be scaled independently based on demand (e.g., scaling the quiz service during peak exam times while maintaining minimal resources for administrative functions).
    
    \item \textbf{Technological Diversity:} Teams can use the best technology for a specific job (e.g., Python for document parsing and machine learning, Node.js for real-time APIs, specialized databases for different data types).
    
    \item \textbf{Improved Resilience:} The failure of a non-critical service does not bring down the entire application, and services can be designed with fallback mechanisms.
    
    \item \textbf{Faster Development Cycles:} Small, independent teams can develop, test, and deploy services independently, accelerating innovation and reducing coordination overhead.
\end{itemize}

Recent systematic reviews have examined both the benefits and challenges of microservice architectures \citep{soldani2023}. While microservices offer significant advantages, they also introduce complexity in terms of service coordination, distributed data management, network latency, and operational overhead. Empirical studies of large-scale microservice systems have identified common issues related to service dependencies, performance degradation, and system complexity \citep{waseem2021}.

Research on microservice anti-patterns has provided valuable guidance for avoiding common pitfalls, such as creating services that are too fine-grained, failing to establish clear service boundaries, or creating tightly coupled services that defeat the purpose of the architecture \citep{taibi2020}. Historical perspectives on microservices demonstrate how the approach has evolved from early service-oriented architectures to modern cloud-native implementations \citep{dragoni2020}.

\section{Review of Different Methodologies}
\label{sec:lit-methodologies}

This section examines the methodological approaches employed in previous research related to e-learning platform development, gamification implementation, AI integration, and system architecture evaluation.

\subsection{Gamification Design Methodologies}

Research on gamification in education has employed various methodological approaches to understand its effectiveness. Systematic reviews and meta-analyses have aggregated findings across multiple studies to identify general patterns and effect sizes \citep{sailer2020, krath2021}. These analyses reveal that gamification's effectiveness depends heavily on contextual factors such as the educational domain, learner characteristics, and the specific game elements employed.

Theoretical research has worked to establish robust frameworks for understanding gamification's mechanisms. Studies have systematically analyzed the theoretical foundations of gamification, serious games, and game-based learning, revealing that Self-Determination Theory is the most commonly cited theoretical framework, though other theories like flow theory, goal-setting theory, and social learning theory also play important roles \citep{krath2021}.

Empirical studies have employed both quantitative and qualitative methods to assess gamification's impact. Quantitative approaches typically measure engagement through metrics like time-on-task, completion rates, and assessment performance. Qualitative approaches use interviews, surveys, and observational studies to understand user perceptions and experiences \citep{dichev2020}.

\subsection{AI Content Generation Evaluation}

Methodologies for evaluating AI-generated educational content typically involve multiple dimensions of quality assessment. Comprehensive surveys of automatic question generation have identified key evaluation criteria including grammatical correctness, semantic coherence, difficulty level appropriateness, distractor quality, and pedagogical value \citep{kurni2023}.

Human expert evaluation remains the gold standard for assessing pedagogical quality, typically involving educators rating generated questions on relevance, clarity, difficulty, and overall quality. However, recent research has also explored automated evaluation metrics and the potential for AI systems to evaluate their own outputs, though these approaches require careful validation \citep{kasneci2023}.

Studies of ChatGPT and similar models in education have employed mixed-methods approaches, combining quantitative performance metrics with qualitative analysis of use cases, limitations, and ethical considerations \citep{baidoo2023}. Framework development research has worked to establish comprehensive models for sustainable AI adoption that address technical, pedagogical, ethical, and institutional dimensions \citep{chiu2024}.

\subsection{Proctoring System Effectiveness}

Research on online proctoring systems has employed various methodologies to assess effectiveness, user acceptance, and privacy implications. Systematic reviews have surveyed the landscape of AI-based proctoring systems, examining their technical approaches, detection capabilities, and limitations \citep{nigam2021}.

Security and privacy analyses have examined the vulnerabilities and risks associated with different proctoring approaches \citep{ullah2021}. These studies often employ threat modeling, security audits, and privacy impact assessments to understand the implications of different technical choices.

User acceptance studies have investigated student and educator perceptions of proctoring systems using surveys, interviews, and experimental studies \citep{gonzalez2023}. These studies reveal significant concerns about privacy, fairness, accessibility, and the psychological impact of surveillance during assessments.

Effectiveness studies typically measure detection accuracy using controlled experiments where participants are instructed to engage in specific behaviors (both legitimate and prohibited) while the system's detection capabilities are assessed \citep{dawson2020}.

\subsection{Microservice Architecture Evaluation}

Research on microservice architectures employs both empirical and analytical methodologies. Empirical studies often involve case studies of real-world systems, examining their evolution, performance characteristics, and operational challenges \citep{waseem2021}. These studies provide valuable insights into the practical realities of microservice adoption and operation.

Performance evaluation typically involves load testing, latency measurement, and resource utilization analysis under various conditions. Comparative studies examine the performance characteristics of monolithic versus microservice architectures for similar applications \citep{soldani2023}.

Qualitative research includes interviews with practitioners, analysis of industry practices, and grey literature reviews of blog posts, conference talks, and technical reports. This research provides insights into real-world challenges, patterns, and anti-patterns that may not be well-documented in academic literature \citep{taibi2020}.

\section{Discussion}
\label{sec:lit-discussion}

\subsection{Comparative Analysis of Existing Platforms}

To position TEKUTOKO within the current EdTech landscape, a comparative analysis of several leading platforms is presented in Table \ref{tab:platform-comparison}.

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{8pt}
\caption{Comparative Analysis of Existing EdTech Platforms}
\label{tab:platform-comparison}
\resizebox{\textwidth}{!}{%
\begin{tabular}{>{\raggedright\arraybackslash}p{2.8cm} 
                >{\centering\arraybackslash}p{3.2cm} 
                >{\centering\arraybackslash}p{3.2cm} 
                >{\centering\arraybackslash}p{3.2cm} 
                >{\centering\arraybackslash}p{3.2cm}}
\toprule
\textbf{Feature} & \textbf{Kahoot!} & \textbf{Quizlet} & \textbf{Google Classroom} & \textbf{TEKUTOKO (Proposed)} \\
\midrule
\textbf{Primary Focus} & 
Live, group-based quizzes & 
Flashcards \& individual study & 
Learning Management System (LMS) & 
Gamified missions, proctored tests, \& community events \\

\textbf{Gamification} & 
High (points, leaderboards) & 
Medium (study games, streaks) & 
Low (assignment tracking) & 
\textbf{Very High} (missions, rewards, vouchers, GPS discovery) \\

\textbf{AI Content Generation} & 
No & 
Yes (Magic Notes for study sets) & 
Limited (plagiarism check) & 
\textbf{Yes} (On-demand question generation from topics \& DOCX) \\

\textbf{Anti-Cheating} & 
No (low-stakes focus) & 
No (self-study focus) & 
Limited (plagiarism detection) & 
\textbf{Yes} (Integrated browser-based proctoring) \\

\textbf{Architecture} & 
Largely Monolithic & 
Microservice-based (partial) & 
Monolithic LMS framework & 
\textbf{Hybrid Microservice} (Node.js core + Python DOCX service) \\
\bottomrule
\end{tabular}%
}
\end{table}

The comparative analysis reveals that existing platforms tend to excel in one or two areas but lack comprehensive integration of all four key technological domains. Kahoot! provides excellent gamification but limited content creation support and no proctoring. Quizlet has incorporated AI for content generation but focuses on individual study rather than instructor-led assessments. Google Classroom provides comprehensive LMS functionality but minimal gamification and basic integrity controls.

\subsection{Identified Research Gaps}

Based on the comprehensive literature review, several critical research gaps have been identified:

\begin{enumerate}
    \item \textbf{Integrated Platform Design:} While individual technologies (gamification, AI content generation, proctoring, microservices) have been well-studied independently, there is limited research on their synergistic integration into a unified e-learning platform. Most existing platforms focus on one or two aspects, leaving opportunities for more comprehensive solutions.
    
    \item \textbf{Lightweight Proctoring Solutions:} Most proctoring research focuses on sophisticated camera-based systems or simple browser locks, with less attention to moderate approaches that balance integrity, privacy, and accessibility. There is a need for empirical validation of browser-based behavioral monitoring as a viable middle-ground approach.
    
    \item \textbf{Practical AI Integration:} While there is extensive research on AI capabilities in education, less attention has been paid to the practical aspects of integrating AI into real-world educational platforms, including prompt engineering strategies, quality assurance workflows, and human-AI collaboration patterns.
    
    \item \textbf{Microservices in EdTech:} While microservice architectures are well-studied in general, their application to educational technology platforms has received limited attention. There is a need for empirical evidence on the performance, scalability, and maintainability benefits in EdTech contexts specifically.
    
    \item \textbf{Holistic Evaluation:} Most studies evaluate single aspects of e-learning platforms (e.g., engagement or content quality) in isolation. There is a need for comprehensive evaluation frameworks that assess multiple dimensions simultaneously, including usability, efficiency, integrity, and technical performance.
\end{enumerate}

\subsection{Theoretical Framework for TEKUTOKO}

Based on the literature review, this research adopts a multi-theoretical framework that integrates:

\begin{itemize}
    \item \textbf{Self-Determination Theory} as the foundation for gamification design, ensuring that game elements support autonomy, competence, and relatedness \citep{deci2020}.
    
    \item \textbf{Human-in-the-Loop AI} principles for content generation, maintaining educator agency and ensuring pedagogical quality while leveraging AI efficiency \citep{kasneci2023}.
    
    \item \textbf{Privacy-Preserving Security} for academic integrity, balancing assessment security with student privacy and accessibility concerns \citep{ullah2021}.
    
    \item \textbf{Pragmatic Microservices} for system architecture, adopting microservice principles where they provide clear benefits while avoiding unnecessary complexity \citep{newman2021}.
\end{itemize}

This integrated framework provides the theoretical foundation for addressing the research gaps identified above and guides the design, implementation, and evaluation of the TEKUTOKO platform as described in subsequent chapters.
