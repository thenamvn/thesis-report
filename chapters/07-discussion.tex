\chapter{Discussion}
\label{chap:discussion}

This chapter provides an in-depth interpretation of the experimental findings presented in Chapter \ref{chap:evaluation}. It synthesizes the results to answer the core research questions, contextualizes the TEKUTOKO platform within the existing body of literature, and offers a critical reflection on the project's strengths, limitations, and the key lessons learned throughout its lifecycle.

\section{Interpretation of Key Findings}
\label{sec:discussion-interpretation}
The evaluation results provide strong, multi-faceted evidence supporting the core hypotheses of this research. The successful integration of gamification, AI content generation, and lightweight proctoring within a scalable architecture has yielded significant, measurable benefits.

The most compelling finding is the profound impact on the user experience and educator workflow. The substantial difference in the System Usability Scale (SUS) scores between the full-featured group (85.5 - "Excellent") and the control group (67.0 - "Marginal") signifies that the synergistic combination of engaging game mechanics and powerful AI tools fundamentally transforms the user's perception of the platform from a simple utility into an enjoyable and efficient ecosystem. This is further substantiated by the \textbf{73.4\% reduction in quiz creation time}. This is a transformative efficiency gain, demonstrating that the AI Question Generator is not a novelty feature but a core utility that directly addresses the critical problem of high content creation overhead for educators.

The performance of the system under load validates the choice of a microservice-based architecture. The ability to maintain response times below 300ms while handling significant concurrent traffic is crucial for a platform that supports real-time, interactive events. This confirms that the architectural design provides a robust and scalable foundation for future growth.

Finally, the high accuracy of the anti-cheating system (96.7\% - 100\% detection rate) demonstrates the viability of a non-invasive, browser-based approach to proctoring. It suggests that for a large number of educational scenarios, it is possible to significantly enhance academic integrity without resorting to the cost, complexity, and privacy concerns associated with more intrusive, camera-based solutions.

\section{Answering the Research Questions}
\label{sec:discussion-questions}
The experimental data provides clear and direct answers to the research questions posed at the outset of this thesis.

\begin{itemize}
    \item \textbf{RQ1 (Engagement \& Usability):} \textit{To what extent does the integration of gamified activities improve user engagement and perceived usability?} The 27.6\% higher SUS score and overwhelmingly positive qualitative feedback from the treatment group provide a definitive answer: the integration of mission-based gamification and reward systems demonstrably and significantly improves both engagement and usability.

    \item \textbf{RQ2 (AI Quality \& Efficiency):} \textit{How effective and reliable is the AI-driven question generation module?} The module is highly effective and reliable. The average expert quality rating of 4.79/5 confirms its ability to produce pedagogically sound content. The 73.4\% reduction in task completion time for hosts quantifies its dramatic impact on educator efficiency.

    \item \textbf{RQ3 (Anti-Cheating Efficacy):} \textit{How effective is the lightweight proctoring system?} The system is highly effective for its intended scope. With detection rates of 96.7-100\% for common digital cheating methods, it provides a reliable and viable mechanism for deterring academic dishonesty in low- to medium-stakes online assessments.

    \item \textbf{RQ4 (Architecture Benefits):} \textit{Does the microservice-based architecture provide quantifiable benefits?} Yes. The load testing results, which show low latency under pressure and a high throughput of 280 RPS with zero errors, provide quantitative evidence of the architecture's performance and scalability, validating its suitability for a modern EdTech application.
\end{itemize}

\section{Comparison with Previous Work}
\label{sec:discussion-comparison}
The TEKUTOKO platform builds upon the foundations laid by pioneering platforms but extends them by integrating their disparate strengths into a single, cohesive framework. While \textbf{Kahoot!} perfected live, low-stakes gamification, it lacks robust content creation tools and features for ensuring academic integrity. While \textbf{Quizlet} has successfully incorporated AI for generating study aids, its focus remains on individual learning rather than on creating secure, instructor-led group assessments. Specialized proctoring services, as reviewed by \citet{ullah2021}, are powerful but often exist as separate, costly products that are not integrated into the learning environment itself.

TEKUTOKO's primary contribution, therefore, is not the invention of any single feature, but the \textbf{synergistic integration} of these three powerful trends—deep gamification, on-demand AI content generation, and accessible proctoring. It provides a unified solution that can be flexibly adapted for a fun, gamified group activity, a collaborative learning mission, or a secure online examination, addressing a significant gap in the current EdTech market.

\section{Strengths and Limitations of the Project}
\label{sec:discussion-strengths-limitations}

\subsection{Strengths}
\begin{itemize}
    \item \textbf{Holistic Integration:} The project's main strength is its successful, end-to-end integration of multiple modern technologies to solve a complex set of interconnected problems in digital education.
    \item \textbf{Practical Utility:} The platform provides clear, measurable benefits to its target users, particularly the significant time savings for educators and the enhanced engagement for learners.
    \item \textbf{User-Centered Design:} A strong focus on a modern, intuitive, and responsive UI/UX was a key factor in the high usability scores and positive user feedback.
    \item \textbf{Scalable and Maintainable Foundation:} The choice of a microservice architecture provides a solid, future-proof foundation for adding new features and handling increased user load.
\end{itemize}

\subsection{Limitations}
\begin{itemize}
    \item \textbf{AI Nuance and Specialization:} The AI's performance was evaluated on general knowledge topics. Its effectiveness in generating high-quality questions for highly specialized, niche, or abstract subjects (e.g., advanced theoretical physics, literary criticism) was not exhaustively tested and may require more sophisticated prompt engineering or model fine-tuning.
    \item \textbf{Scope of Proctoring:} The anti-cheating system is intentionally limited to browser-based events. It cannot prevent cheating that occurs "off-screen," such as a student using a second device (e.g., a phone) or receiving in-person assistance. It serves as a deterrent, not a foolproof guarantee of integrity.
    \item \textbf{Generalizability of User Study:} The evaluation was conducted with a specific demographic (university technology students). The findings regarding usability and engagement may not be fully generalizable to other user groups, such as younger K-12 students or non-technical corporate trainees, without further study.
\end{itemize}

\section{Lessons Learned and Reflections}
\label{sec:discussion-lessons}
The development and evaluation of the TEKUTOKO platform yielded several important insights:
\begin{itemize}
    \item \textbf{The Critical Role of Prompt Engineering:} The quality of the AI-generated content was directly proportional to the quality and specificity of the prompt sent to the Gemini API. Crafting a detailed, structured prompt that explicitly requested a JSON output was the most critical factor in achieving reliable and easily parsable results.
    \item \textbf{Balancing Security and User Experience:} Implementing the anti-cheating features required a delicate balance. Overly aggressive measures could frustrate honest users experiencing technical glitches, while overly lax measures would be ineffective. The implemented warning system represents a pragmatic compromise.
    \item \textbf{The Indispensable "Human-in-the-Loop":} The evaluation confirmed that even with a powerful AI model, human oversight is essential for critical applications like education. The design decision to make the AI a "co-pilot" for the educator, with a mandatory review and approval step, was crucial for ensuring content quality and accuracy.
    \item \textbf{Pragmatic Microservices:} For a project of this scale, a pragmatic approach to microservices—separating only the most distinct and computationally different components (like the DOCX parser)—provided the benefits of modularity without the full operational overhead of a more granular microservice ecosystem.
\end{itemize}